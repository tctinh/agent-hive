/**
 * AUTO-GENERATED FILE - DO NOT EDIT
 * Generated by: scripts/generate-skills.ts
 * Run: bun run scripts/generate-skills.ts
 */

import type { SkillDefinition } from './types.js';

/**
 * List of builtin skill names.
 */
export const BUILTIN_SKILL_NAMES = ["brainstorming", "code-reviewer", "dispatching-parallel-agents", "executing-plans", "onboarding", "parallel-exploration", "systematic-debugging", "test-driven-development", "verification-before-completion", "writing-plans"] as const;

/**
 * All builtin skill definitions.
 */
export const BUILTIN_SKILLS: SkillDefinition[] = [
  {
    name: "brainstorming",
    description: "Use before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores user intent, requirements and design before implementation.",
    template: "# Brainstorming Ideas Into Designs\n\n## Overview\n\nHelp turn ideas into fully formed designs and specs through natural collaborative dialogue.\n\nStart by understanding the current project context, then ask questions one at a time to refine the idea. Once you understand what you're building, present the design in small sections (200-300 words), checking after each section whether it looks right so far.\n\n## The Process\n\n**Understanding the idea:**\n- Check out the current project state first (files, docs, recent commits)\n- Ask questions one at a time to refine the idea\n- Prefer multiple choice questions when possible, but open-ended is fine too\n- Only one question per message - if a topic needs more exploration, break it into multiple questions\n- Focus on understanding: purpose, constraints, success criteria\n\n**Exploring approaches:**\n- Propose 2-3 different approaches with trade-offs\n- Present options conversationally with your recommendation and reasoning\n- Lead with your recommended option and explain why\n\n**Presenting the design:**\n- Once you believe you understand what you're building, present the design\n- Break it into sections of 200-300 words\n- Ask after each section whether it looks right so far\n- Cover: architecture, components, data flow, error handling, testing\n- Be ready to go back and clarify if something doesn't make sense\n\n## After the Design\n\n**Documentation:**\n- Write the validated design to `docs/plans/YYYY-MM-DD-<topic>-design.md`\n- Commit the design document to git\n\n**Implementation (if continuing):**\n- Ask: \"Ready to set up for implementation?\"\n- Use `hive_skill:writing-plans` to create detailed implementation plan\n\n## Key Principles\n\n- **One question at a time** - Don't overwhelm with multiple questions\n- **Multiple choice preferred** - Easier to answer than open-ended when possible\n- **YAGNI ruthlessly** - Remove unnecessary features from all designs\n- **Explore alternatives** - Always propose 2-3 approaches before settling\n- **Incremental validation** - Present design in sections, validate each\n- **Be flexible** - Go back and clarify when something doesn't make sense",
  },
  {
    name: "code-reviewer",
    description: "Use when reviewing implementation changes against an approved plan or task (especially before merging or between Hive tasks) to catch missing requirements, YAGNI, dead code, and risky patterns",
    template: "# Code Reviewer\n\n## Overview\n\nThis skill teaches a reviewer to evaluate implementation changes for:\n- Adherence to the approved plan/task (did we build what we said?)\n- Correctness (does it work, including edge cases?)\n- Simplicity (YAGNI, dead code, over-abstraction)\n- Risk (security, performance, maintainability)\n\n**Core principle:** The best change is the smallest correct change that satisfies the plan.\n\n## Iron Laws\n\n- Review against the task/plan first. Code quality comes second.\n- Bias toward deletion and simplification. Every extra line is a liability.\n- Prefer changes that leverage existing patterns and dependencies.\n- Be specific: cite file paths and (when available) line numbers.\n- Do not invent requirements. If the plan/task is ambiguous, mark it and request clarification.\n\n## What Inputs You Need\n\nMinimum:\n- The task intent (1-3 sentences)\n- The plan/task requirements (or a link/path to plan section)\n- The code changes (diff or list of changed files)\n\nIf available (recommended):\n- Acceptance criteria / verification steps from the plan\n- Test output or proof the change was verified\n- Any relevant context files (design decisions, constraints)\n\n## Review Process (In Order)\n\n### 1) Identify Scope\n\n1. List all files changed.\n2. For each file, state why it changed (what requirement it serves).\n3. Flag any changes that do not map to the task/plan.\n\n**Rule:** If you cannot map a change to a requirement, treat it as suspicious until justified.\n\n### 2) Plan/Task Adherence (Non-Negotiable)\n\nCreate a simple checklist:\n- What the task says must happen\n- Evidence in code/tests that it happens\n\nFlag as issues:\n- Missing requirements (implemented behavior does not match intent)\n- Partial implementation with no follow-up task (TODO-driven shipping)\n- Behavior changes that are not in the plan/task\n\n### 3) Correctness Layer\n\nReview for:\n- Edge cases and error paths\n- Incorrect assumptions about inputs/types\n- Inconsistent behavior across platforms/environments\n- Broken invariants (e.g., state can become invalid)\n\nPrefer \"fail fast, fail loud\": invalid states should become clear errors, not silent fallbacks.\n\n### 4) Simplicity / YAGNI Layer\n\nBe ruthless and concrete:\n- Remove dead branches, unused flags/options, unreachable code\n- Remove speculative TODOs and \"reserved for future\" scaffolding\n- Remove comments that restate the code or narrate obvious steps\n- Inline one-off abstractions (helpers/classes/interfaces used once)\n- Replace cleverness with obvious code\n- Reduce nesting with guard clauses / early returns\n\nPrefer clarity over brevity:\n- Avoid nested ternary operators; use `if/else` or `switch` when branches matter\n- Avoid dense one-liners that hide intent or make debugging harder\n\n### 4b) De-Slop Pass (AI Artifacts / Style Drift)\n\nScan the diff (not just the final code) for AI-generated slop introduced in this branch:\n- Extra comments that a human would not add, or that do not match the file's tone\n- Defensive checks or try/catch blocks that are abnormal for that area of the codebase\n  - Especially swallowed errors (\"ignore and continue\") and silent fallbacks\n  - Especially redundant validation in trusted internal codepaths\n- TypeScript escape hatches used to dodge type errors (`as any`, `as unknown as X`) without necessity\n- Style drift: naming, error handling patterns, logging style, and structure inconsistent with nearby code\n\nDefault stance:\n- Prefer deletion over justification.\n- If validation is needed, do it at boundaries; keep internals trusting parsed inputs.\n- If a cast is truly unavoidable, localize it and keep the justification to a single short note.\n\nWhen recommending simplifications, do not accidentally change behavior. If the current behavior is unclear, request clarification or ask for a test that pins it down.\n\n**Default stance:** Do not add extensibility points without an explicit current requirement.\n\n### 5) Risk Layer (Security / Performance / Maintainability)\n\nOnly report what you are confident about.\n\nSecurity checks (examples):\n- No secrets in code/logs\n- No injection vectors (shell/SQL/HTML) introduced\n- Authz/authn checks preserved\n- Sensitive data not leaked\n\nPerformance checks (examples):\n- Avoid unnecessary repeated work (N+1 queries, repeated parsing, repeated filesystem hits)\n- Avoid obvious hot-path allocations or large sync operations\n\nMaintainability checks:\n- Clear naming and intent\n- Consistent error handling\n- API boundaries not blurred\n- Consistent with local file patterns (imports, export style, function style)\n\n### 6) Make One Primary Recommendation\n\nProvide one clear path to reach approval.\nMention alternatives only when they have materially different trade-offs.\n\n### 7) Signal the Investment\n\nTag the required follow-up effort using:\n- Quick (<1h)\n- Short (1-4h)\n- Medium (1-2d)\n- Large (3d+)\n\n## Confidence Filter\n\nOnly report findings you believe are >=80% likely to be correct.\nIf you are unsure, explicitly label it as \"Uncertain\" and explain what evidence would confirm it.\n\n## Output Format (Use This Exactly)\n\n---\n\n**Files Reviewed:** [list]\n\n**Plan/Task Reference:** [task name + link/path to plan section if known]\n\n**Overall Assessment:** [APPROVE | REQUEST_CHANGES | NEEDS_DISCUSSION]\n\n**Bottom Line:** 2-3 sentences describing whether it matches the task/plan and what must change.\n\n### Critical Issues\n- None | [file:line] - [issue] (why it blocks approval) + (recommended fix)\n\n### Major Issues\n- None | [file:line] - [issue] + (recommended fix)\n\n### Minor Issues\n- None | [file:line] - [issue] + (suggested fix)\n\n### YAGNI / Dead Code\n- None | [file:line] - [what to remove/simplify] + (why it is unnecessary)\n\n### Positive Observations\n- [at least one concrete good thing]\n\n### Action Plan\n1. [highest priority change]\n2. [next]\n3. [next]\n\n### Effort Estimate\n[Quick | Short | Medium | Large]\n\n---\n\n## Common Review Smells (Fast Scan)\n\nTask/plan adherence:\n- Adds features not mentioned in the plan/task\n- Leaves TODOs as the mechanism for correctness\n- Introduces new configuration modes/flags \"for future\"\n\nYAGNI / dead code:\n- Options/config that are parsed but not used\n- Branches that do the same thing on both sides\n- Comments like \"reserved for future\" or \"we might need this\"\n\nAI slop / inconsistency:\n- Commentary that restates code, narrates obvious steps, or adds process noise\n- try/catch that swallows errors or returns defaults without a requirement\n- `as any` used to silence type errors instead of fixing types\n- New helpers/abstractions with a single call site\n\nCorrectness:\n- Silent fallbacks to defaults on error when the task expects a hard failure\n- Unhandled error paths, missing cleanup, missing returns\n\nMaintainability:\n- Abstractions used once\n- Unclear naming, \"utility\" grab-bags\n\n## When to Escalate\n\nUse NEEDS_DISCUSSION (instead of REQUEST_CHANGES) when:\n- The plan/task is ambiguous and multiple implementations could be correct\n- The change implies a product/architecture decision not documented\n- Fixing issues requires changing scope, dependencies, or public API",
  },
  {
    name: "dispatching-parallel-agents",
    description: "Use when facing 2+ independent tasks that can be worked on without shared state or sequential dependencies",
    template: "# Dispatching Parallel Agents\n\n## Overview\n\nWhen you have multiple unrelated failures (different test files, different subsystems, different bugs), investigating them sequentially wastes time. Each investigation is independent and can happen in parallel.\n\n**Core principle:** Dispatch one agent per independent problem domain. Let them work concurrently.\n\n## Prerequisite: Check Runnable Tasks\n\nBefore dispatching, use `hive_status()` to get the **runnable** list — tasks whose dependencies are all satisfied.\n\n**Only dispatch tasks that are runnable.** Never start tasks with unmet dependencies.\n\nOnly `done` satisfies dependencies (not `blocked`, `failed`, `partial`, `cancelled`).\n\n**Ask the operator first:**\n- Use `question()`: \"These tasks are runnable and independent: [list]. Execute in parallel?\"\n- Record the decision with `hive_context_write({ name: \"execution-decisions\", content: \"...\" })`\n- Proceed only after operator approval\n\n## When to Use\n\n```dot\ndigraph when_to_use {\n    \"Multiple failures?\" [shape=diamond];\n    \"Are they independent?\" [shape=diamond];\n    \"Single agent investigates all\" [shape=box];\n    \"One agent per problem domain\" [shape=box];\n    \"Can they work in parallel?\" [shape=diamond];\n    \"Sequential agents\" [shape=box];\n    \"Parallel dispatch\" [shape=box];\n\n    \"Multiple failures?\" -> \"Are they independent?\" [label=\"yes\"];\n    \"Are they independent?\" -> \"Single agent investigates all\" [label=\"no - related\"];\n    \"Are they independent?\" -> \"Can they work in parallel?\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Parallel dispatch\" [label=\"yes\"];\n    \"Can they work in parallel?\" -> \"Sequential agents\" [label=\"no - shared state\"];\n}\n```\n\n**Use when:**\n- 3+ test files failing with different root causes\n- Multiple subsystems broken independently\n- Each problem can be understood without context from others\n- No shared state between investigations\n\n**Don't use when:**\n- Failures are related (fix one might fix others)\n- Need to understand full system state\n- Agents would interfere with each other\n\n## The Pattern\n\n### 1. Identify Independent Domains\n\nGroup failures by what's broken:\n- File A tests: Tool approval flow\n- File B tests: Batch completion behavior\n- File C tests: Abort functionality\n\nEach domain is independent - fixing tool approval doesn't affect abort tests.\n\n### 2. Create Focused Agent Tasks\n\nEach agent gets:\n- **Specific scope:** One test file or subsystem\n- **Clear goal:** Make these tests pass\n- **Constraints:** Don't change other code\n- **Expected output:** Summary of what you found and fixed\n\n### 3. Dispatch in Parallel\n\n```typescript\n// Using Hive tools for parallel execution\nhive_exec_start({ task: \"01-fix-abort-tests\" })\nhive_exec_start({ task: \"02-fix-batch-tests\" })\nhive_exec_start({ task: \"03-fix-race-condition-tests\" })\n// All three run concurrently in isolated worktrees\n```\n\nParallelize by issuing multiple task() calls in the same assistant message.\n\n```typescript\ntask({ subagent_type: 'scout-researcher', prompt: 'Investigate failure A' })\ntask({ subagent_type: 'scout-researcher', prompt: 'Investigate failure B' })\n```\n\n### 4. Review and Integrate\n\nWhen agents return:\n- Read each summary\n- Verify fixes don't conflict\n- Run full test suite\n- Integrate all changes with `hive_merge`\n\n## Agent Prompt Structure\n\nGood agent prompts are:\n1. **Focused** - One clear problem domain\n2. **Self-contained** - All context needed to understand the problem\n3. **Specific about output** - What should the agent return?\n\n```markdown\nFix the 3 failing tests in src/agents/agent-tool-abort.test.ts:\n\n1. \"should abort tool with partial output capture\" - expects 'interrupted at' in message\n2. \"should handle mixed completed and aborted tools\" - fast tool aborted instead of completed\n3. \"should properly track pendingToolCount\" - expects 3 results but gets 0\n\nThese are timing/race condition issues. Your task:\n\n1. Read the test file and understand what each test verifies\n2. Identify root cause - timing issues or actual bugs?\n3. Fix by:\n   - Replacing arbitrary timeouts with event-based waiting\n   - Fixing bugs in abort implementation if found\n   - Adjusting test expectations if testing changed behavior\n\nDo NOT just increase timeouts - find the real issue.\n\nReturn: Summary of what you found and what you fixed.\n```\n\n## Common Mistakes\n\n**❌ Too broad:** \"Fix all the tests\" - agent gets lost\n**✅ Specific:** \"Fix agent-tool-abort.test.ts\" - focused scope\n\n**❌ No context:** \"Fix the race condition\" - agent doesn't know where\n**✅ Context:** Paste the error messages and test names\n\n**❌ No constraints:** Agent might refactor everything\n**✅ Constraints:** \"Do NOT change production code\" or \"Fix tests only\"\n\n**❌ Vague output:** \"Fix it\" - you don't know what changed\n**✅ Specific:** \"Return summary of root cause and changes\"\n\n## When NOT to Use\n\n**Related failures:** Fixing one might fix others - investigate together first\n**Need full context:** Understanding requires seeing entire system\n**Exploratory debugging:** You don't know what's broken yet\n**Shared state:** Agents would interfere (editing same files, using same resources)\n\n## Real Example from Session\n\n**Scenario:** 6 test failures across 3 files after major refactoring\n\n**Failures:**\n- agent-tool-abort.test.ts: 3 failures (timing issues)\n- batch-completion-behavior.test.ts: 2 failures (tools not executing)\n- tool-approval-race-conditions.test.ts: 1 failure (execution count = 0)\n\n**Decision:** Independent domains - abort logic separate from batch completion separate from race conditions\n\n**Dispatch:**\n```\nAgent 1 → Fix agent-tool-abort.test.ts\nAgent 2 → Fix batch-completion-behavior.test.ts\nAgent 3 → Fix tool-approval-race-conditions.test.ts\n```\n\n**Results:**\n- Agent 1: Replaced timeouts with event-based waiting\n- Agent 2: Fixed event structure bug (threadId in wrong place)\n- Agent 3: Added wait for async tool execution to complete\n\n**Integration:** All fixes independent, no conflicts, full suite green\n\n**Time saved:** 3 problems solved in parallel vs sequentially\n\n## Key Benefits\n\n1. **Parallelization** - Multiple investigations happen simultaneously\n2. **Focus** - Each agent has narrow scope, less context to track\n3. **Independence** - Agents don't interfere with each other\n4. **Speed** - 3 problems solved in time of 1\n\n## Verification\n\nAfter agents return:\n1. **Review each summary** - Understand what changed\n2. **Check for conflicts** - Did agents edit same code?\n3. **Run full suite** - Verify all fixes work together\n4. **Spot check** - Agents can make systematic errors\n\n## Real-World Impact\n\nFrom debugging session (2025-10-03):\n- 6 failures across 3 files\n- 3 agents dispatched in parallel\n- All investigations completed concurrently\n- All fixes integrated successfully\n- Zero conflicts between agent changes",
  },
  {
    name: "executing-plans",
    description: "Use when you have a written implementation plan to execute in a separate session with review checkpoints",
    template: "# Executing Plans\n\n## Overview\n\nLoad plan, review critically, execute tasks in batches, report for review between batches.\n\n**Core principle:** Batch execution with checkpoints for architect review.\n\n**Announce at start:** \"I'm using the executing-plans skill to implement this plan.\"\n\n## The Process\n\n### Step 1: Load and Review Plan\n1. Read plan file\n2. Review critically - identify any questions or concerns about the plan\n3. If concerns: Raise them with your human partner before starting\n4. If no concerns: Create TodoWrite and proceed\n\n### Step 2: Identify Runnable Tasks\n\nUse `hive_status()` to get the **runnable** list — tasks with all dependencies satisfied.\n\nOnly `done` satisfies dependencies (not `blocked`, `failed`, `partial`, `cancelled`).\n\n**When 2+ tasks are runnable:**\n- **Ask the operator** via `question()`: \"Multiple tasks are runnable: [list]. Run in parallel, sequential, or a specific subset?\"\n- Record the decision with `hive_context_write({ name: \"execution-decisions\", content: \"...\" })` for future reference\n\n**When 1 task is runnable:** Proceed directly.\n\n### Step 3: Execute Batch\n\nFor each task in the batch:\n1. Mark as in_progress via `hive_exec_start()`\n2. Follow each step exactly (plan has bite-sized steps)\n3. Run verifications as specified\n4. Mark as completed\n\n### Step 4: Report\nWhen batch complete:\n- Show what was implemented\n- Show verification output\n- Say: \"Ready for feedback.\"\n\n### Step 5: Continue\nBased on feedback:\n- Apply changes if needed\n- Execute next batch\n- Repeat until complete\n\n### Step 6: Complete Development\n\nAfter all tasks complete and verified:\n- Announce: \"I'm using the finishing-a-development-branch skill to complete this work.\"\n- **REQUIRED SUB-SKILL:** Use hive_skill:finishing-a-development-branch\n- Follow that skill to verify tests, present options, execute choice\n\n## When to Stop and Ask for Help\n\n**STOP executing immediately when:**\n- Hit a blocker mid-batch (missing dependency, test fails, instruction unclear)\n- Plan has critical gaps preventing starting\n- You don't understand an instruction\n- Verification fails repeatedly\n\n**Ask for clarification rather than guessing.**\n\n## When to Revisit Earlier Steps\n\n**Return to Review (Step 1) when:**\n- Partner updates the plan based on your feedback\n- Fundamental approach needs rethinking\n\n**Don't force through blockers** - stop and ask.\n\n## Remember\n- Review plan critically first\n- Follow plan steps exactly\n- Don't skip verifications\n- Reference skills when plan says to\n- Between batches: just report and wait\n- Stop when blocked, don't guess",
  },
  {
    name: "onboarding",
    description: "Ask about workflow preferences and store them in .hive/contexts/preferences.md before proceeding.",
    template: "# Onboarding Preferences\n\n## Overview\n\nGather workflow preferences so the assistant can match the user's desired working style.\n\n## When to Ask\n\n- **Immediately when the skill is loaded**, before any other work.\n- If `.hive/contexts/preferences.md` does not exist, start onboarding.\n- If later a decision is ambiguous and preferences are missing, ask again.\n\n## Preference Storage\n\nUse `hive_context_write` to write `.hive/contexts/preferences.md` with this exact template:\n\n```\n# Preferences\n\n## Exploration Style\nsync\n\n## Research Depth\nmedium\n\n## Confirmation Level\nstandard\n\n## Commit Behavior\nask-before-commit\n```\n\n## If Preferences Already Exist\n\nFollow the same pattern used in `packages/vscode-hive/src/tools/plan.ts`:\n\n1. Use `contextService.list(feature)` to detect existing contexts.\n2. Ask **\"Preferences already exist. Keep or overwrite?\"** using the `question()` tool.\n3. If keep → continue using existing preferences.\n4. If overwrite → collect new answers and write them with `hive_context_write`.\n\n## Questions to Ask (Always use `question()`)\n\nAsk one at a time, with the provided options. Store the answers in `.hive/contexts/preferences.md`.\n\n1. **Exploration Style:** sync | async\n2. **Research Depth:** shallow | medium | deep\n3. **Confirmation Level:** minimal | standard | high\n4. **Commit Behavior:** ask-before-commit | auto-commit | never-commit\n\n## Requirements\n\n- Use the `question()` tool (no plain text questions).\n- Ask immediately when the skill loads if preferences are missing.\n- If later a decision is ambiguous and preferences are missing, ask again.\n- Always store answers using `hive_context_write` with the template above.",
  },
  {
    name: "parallel-exploration",
    description: "Use when you need parallel, read-only exploration with hive_background_* or task() (Scout fan-out)",
    template: "# Parallel Exploration (Scout Fan-Out)\n\n## Overview\n\nWhen you need to answer \"where/how does X work?\" across multiple domains (codebase, tests, docs, OSS), investigating sequentially wastes time. Each investigation is independent and can happen in parallel.\n\n**Core principle:** Decompose into independent sub-questions, spawn one task per sub-question, collect results asynchronously.\n\n**Safe in Planning mode:** This is read-only exploration. It is OK to use during exploratory research even when there is no feature, no plan, and no approved tasks.\n\n**This skill is for read-only research.** For parallel implementation work, use `hive_skill(\"dispatching-parallel-agents\")` with `hive_exec_start`.\n\n**Two valid execution paths:**\n- **Path A (Hive background tools):** Use `hive_background_task`, `hive_background_output`, `hive_background_cancel` when available.\n- **Path B (Task mode):** Use native `task()` for delegation when background tools are not registered.\n\n## When to Use\n\n**Default to this skill when:**\n**Use when:**\n- Investigation spans multiple domains (code + tests + docs)\n- User asks **2+ questions across different domains** (e.g., code + tests, code + docs/OSS, code + config/runtime)\n- Questions are independent (answer to A doesn't affect B)\n- User asks **3+ independent questions** (often as a numbered list or separate bullets)\n- No edits needed (read-only exploration)\n- User asks for an explorationthat likely spans multiple files/packages\n- The work is read-only and the questions can be investigated independently\n\n**Only skip this skill when:**\n- Investigation requires shared state or context between questions\n- It's a single focused question that is genuinely answerable with **one quick grep + one file read**\n- Questions are dependent (answer A materially changes what to ask for B)\n- Work involves file edits (use Hive tasks / Forager instead)\n\n**Important:** Do not treat \"this is exploratory\" as a reason to avoid delegation. This skill is specifically for exploratory research when fan-out makes it faster and cleaner.\n\n## The Pattern\n\n### 1. Decompose Into Independent Questions\n\nSplit your investigation into 2-4 independent sub-questions. Good decomposition:\n\n| Domain | Question Example |\n|--------|------------------|\n| Codebase | \"Where is X implemented? What files define it?\" |\n| Tests | \"How is X tested? What test patterns exist?\" |\n| Docs/OSS | \"How do other projects implement X? What's the recommended pattern?\" |\n| Config | \"How is X configured? What environment variables affect it?\" |\n\n**Bad decomposition (dependent questions):**\n- \"What is X?\" then \"How is X used?\" (second depends on first)\n- \"Find the bug\" then \"Fix the bug\" (not read-only)\n\n### 2. Spawn Background Tasks (Fan-Out)\n\nLaunch all tasks before waiting for any results:\n\n```typescript\n// Path A: Hive background tools (when available)\n// Fan-out: spawn all tasks first\nhive_background_task({\n  agent: \"scout-researcher\",\n  description: \"Find hive_background_task implementation\",\n  prompt: `Where is hive_background_task implemented and registered?\n    - Find the tool definition\n    - Find the plugin registration\n    - Return file paths with line numbers`,\n  sync: false\n})\n\nhive_background_task({\n  agent: \"scout-researcher\",\n  description: \"Analyze background task concurrency\",\n  prompt: `How does background task concurrency/queueing work?\n    - Find the manager/scheduler code\n    - Document the concurrency model\n    - Return file paths with evidence`,\n  sync: false\n})\n\nhive_background_task({\n  agent: \"scout-researcher\",\n  description: \"Find parent notification mechanism\",\n  prompt: `How does parent notification work for background tasks?\n    - Where is the notification built?\n    - How is it sent to the parent session?\n    - Return file paths with evidence`,\n  sync: false\n})\n```\n\n```typescript\n// Path B: Task mode (native task tool)\n// Parallelize by issuing multiple task() calls in the same assistant message.\ntask({\n  subagent_type: 'scout-researcher',\n  description: 'Find hive_background_task implementation',\n  prompt: `Where is hive_background_task implemented and registered?\n    - Find the tool definition\n    - Find the plugin registration\n    - Return file paths with line numbers`,\n});\n\ntask({\n  subagent_type: 'scout-researcher',\n  description: 'Analyze background task concurrency',\n  prompt: `How does background task concurrency/queueing work?\n    - Find the manager/scheduler code\n    - Document the concurrency model\n    - Return file paths with evidence`,\n});\n\ntask({\n  subagent_type: 'scout-researcher',\n  description: 'Find parent notification mechanism',\n  prompt: `How does parent notification work for background tasks?\n    - Where is the notification built?\n    - How is it sent to the parent session?\n    - Return file paths with evidence`,\n});\n```\n\n**Key points:**\n- Use `agent: \"scout-researcher\"` for read-only exploration\n- Use `sync: false` to return immediately (non-blocking)\n- Give each task a clear, focused `description`\n- Make prompts specific about what evidence to return\n\n### 3. Continue Working (Optional)\n\nWhile tasks run, you can:\n- Work on other aspects of the problem\n- Prepare synthesis structure\n- Start drafting based on what you already know\n\nYou'll receive a `<system-reminder>` notification when each task completes.\n\n### 4. Collect Results\n\nWhen notified of completion, retrieve results:\n\n```typescript\n// Path A: Hive background tools\n// Get output from completed task\nhive_background_output({\n  task_id: \"task-abc123\",\n  block: false  // Don't wait, task already done\n})\n```\n\n**For incremental output (long-running tasks):**\n\n```typescript\n// First call - get initial output\nhive_background_output({\n  task_id: \"task-abc123\",\n  block: true,      // Wait for output\n  timeout: 30000    // 30 second timeout\n})\n// Returns: { output: \"...\", cursor: \"5\" }\n\n// Later call - get new output since cursor\nhive_background_output({\n  task_id: \"task-abc123\",\n  cursor: \"5\",      // Resume from message 5\n  block: true\n})\n```\n\n### 5. Synthesize Findings\n\nCombine results from all tasks:\n- Cross-reference findings (file X mentioned by tasks A and B)\n- Identify gaps (task C found nothing, need different approach)\n- Build coherent answer from parallel evidence\n\n### 6. Cleanup (If Needed)\n\nCancel tasks that are no longer needed:\n\n```typescript\n// Path A: Hive background tools\n// Cancel specific task\nhive_background_cancel({ task_id: \"task-abc123\" })\n\n// Cancel all your background tasks\nhive_background_cancel({ all: true })\n```\n\n## Prompt Templates\n\n### Codebase Slice\n\n```\nInvestigate [TOPIC] in the codebase:\n- Where is [X] defined/implemented?\n- What files contain [X]?\n- How does [X] interact with [Y]?\n\nReturn:\n- File paths with line numbers\n- Brief code snippets as evidence\n- Key patterns observed\n```\n\n### Tests Slice\n\n```\nInvestigate how [TOPIC] is tested:\n- What test files cover [X]?\n- What testing patterns are used?\n- What edge cases are tested?\n\nReturn:\n- Test file paths\n- Example test patterns\n- Coverage gaps if obvious\n```\n\n### Docs/OSS Slice\n\n```\nResearch [TOPIC] in external sources:\n- How do other projects implement [X]?\n- What does the official documentation say?\n- What are common patterns/anti-patterns?\n\nReturn:\n- Links to relevant docs/repos\n- Key recommendations\n- Patterns that apply to our codebase\n```\n\n## Real Example\n\n**Investigation:** \"How does the background task system work?\"\n\n**Decomposition:**\n1. Implementation: Where is `hive_background_task` tool defined?\n2. Concurrency: How does task scheduling/queueing work?\n3. Notifications: How does parent session get notified?\n\n**Fan-out:**\n```typescript\n// Path A: Hive background tools\n// Task 1: Implementation\nhive_background_task({\n  agent: \"scout-researcher\",\n  description: \"Find hive_background_task implementation\",\n  prompt: \"Where is hive_background_task implemented? Find tool definition and registration.\",\n  sync: false\n})\n\n// Task 2: Concurrency\nhive_background_task({\n  agent: \"scout-researcher\",\n  description: \"Analyze concurrency model\",\n  prompt: \"How does background task concurrency work? Find the manager/scheduler.\",\n  sync: false\n})\n\n// Task 3: Notifications\nhive_background_task({\n  agent: \"scout-researcher\",\n  description: \"Find notification mechanism\",\n  prompt: \"How are parent sessions notified of task completion?\",\n  sync: false\n})\n```\n\n```typescript\n// Path B: Task mode (native task tool)\n// Parallelize by issuing multiple task() calls in the same assistant message.\ntask({\n  subagent_type: 'scout-researcher',\n  description: 'Find hive_background_task implementation',\n  prompt: 'Where is hive_background_task implemented? Find tool definition and registration.',\n});\n\ntask({\n  subagent_type: 'scout-researcher',\n  description: 'Analyze concurrency model',\n  prompt: 'How does background task concurrency work? Find the manager/scheduler.',\n});\n\ntask({\n  subagent_type: 'scout-researcher',\n  description: 'Find notification mechanism',\n  prompt: 'How are parent sessions notified of task completion?',\n});\n```\n\n**Results:**\n- Task 1: Found `background-tools.ts` (tool definition), `index.ts` (registration)\n- Task 2: Found `manager.ts` with concurrency=3 default, queue-based scheduling\n- Task 3: Found `session.prompt()` call in manager for parent notification\n\n**Synthesis:** Complete picture of background task lifecycle in ~1/3 the time of sequential investigation.\n\n## Common Mistakes\n\n**Spawning sequentially (defeats the purpose):**\n```typescript\n// BAD: Wait for each before spawning next\nconst result1 = await hive_background_task({ ..., sync: true })\nconst result2 = await hive_background_task({ ..., sync: true })\n```\n\n```typescript\n// GOOD: Spawn all, then collect\nhive_background_task({ ..., sync: false })  // Returns immediately\nhive_background_task({ ..., sync: false })  // Returns immediately\nhive_background_task({ ..., sync: false })  // Returns immediately\n// ... later, collect results with hive_background_output\n```\n\n```typescript\n// GOOD (task mode): Spawn all in the same assistant message\ntask({ ... });\ntask({ ... });\ntask({ ... });\n```\n\n**Too many tasks (diminishing returns):**\n- 2-4 tasks: Good parallelization\n- 5+ tasks: Overhead exceeds benefit, harder to synthesize\n\n**Dependent questions:**\n- Don't spawn task B if it needs task A's answer\n- Either make them independent or run sequentially\n\n**Using for edits:**\n- Scout is read-only; use Forager for implementation\n- This skill is for exploration, not execution\n\n## Key Benefits\n\n1. **Speed** - 3 investigations in time of 1\n2. **Focus** - Each Scout has narrow scope\n3. **Independence** - No interference between tasks\n4. **Flexibility** - Cancel unneeded tasks, add new ones\n\n## Verification\n\nAfter using this pattern, verify:\n- [ ] All tasks spawned before collecting any results (true fan-out)\n- [ ] Received notifications for completed tasks (Path A)\n- [ ] Successfully retrieved output with `hive_background_output` (Path A)\n- [ ] Verified `task()` fan-out pattern used when in task mode (Path B)\n- [ ] Synthesized findings into coherent answer",
  },
  {
    name: "systematic-debugging",
    description: "Use when encountering any bug, test failure, or unexpected behavior, before proposing fixes",
    template: "# Systematic Debugging\n\n## Overview\n\nRandom fixes waste time and create new bugs. Quick patches mask underlying issues.\n\n**Core principle:** ALWAYS find root cause before attempting fixes. Symptom fixes are failure.\n\n**Violating the letter of this process is violating the spirit of debugging.**\n\n## The Iron Law\n\n```\nNO FIXES WITHOUT ROOT CAUSE INVESTIGATION FIRST\n```\n\nIf you haven't completed Phase 1, you cannot propose fixes.\n\n## When to Use\n\nUse for ANY technical issue:\n- Test failures\n- Bugs in production\n- Unexpected behavior\n- Performance problems\n- Build failures\n- Integration issues\n\n**Use this ESPECIALLY when:**\n- Under time pressure (emergencies make guessing tempting)\n- \"Just one quick fix\" seems obvious\n- You've already tried multiple fixes\n- Previous fix didn't work\n- You don't fully understand the issue\n\n**Don't skip when:**\n- Issue seems simple (simple bugs have root causes too)\n- You're in a hurry (rushing guarantees rework)\n- Manager wants it fixed NOW (systematic is faster than thrashing)\n\n## The Four Phases\n\nYou MUST complete each phase before proceeding to the next.\n\n### Phase 1: Root Cause Investigation\n\n**BEFORE attempting ANY fix:**\n\n1. **Read Error Messages Carefully**\n   - Don't skip past errors or warnings\n   - They often contain the exact solution\n   - Read stack traces completely\n   - Note line numbers, file paths, error codes\n\n2. **Reproduce Consistently**\n   - Can you trigger it reliably?\n   - What are the exact steps?\n   - Does it happen every time?\n   - If not reproducible → gather more data, don't guess\n\n3. **Check Recent Changes**\n   - What changed that could cause this?\n   - Git diff, recent commits\n   - New dependencies, config changes\n   - Environmental differences\n\n4. **Gather Evidence in Multi-Component Systems**\n\n   **WHEN system has multiple components (CI → build → signing, API → service → database):**\n\n   **BEFORE proposing fixes, add diagnostic instrumentation:**\n   ```\n   For EACH component boundary:\n     - Log what data enters component\n     - Log what data exits component\n     - Verify environment/config propagation\n     - Check state at each layer\n\n   Run once to gather evidence showing WHERE it breaks\n   THEN analyze evidence to identify failing component\n   THEN investigate that specific component\n   ```\n\n   **Example (multi-layer system):**\n   ```bash\n   # Layer 1: Workflow\n   echo \"=== Secrets available in workflow: ===\"\n   echo \"IDENTITY: ${IDENTITY:+SET}${IDENTITY:-UNSET}\"\n\n   # Layer 2: Build script\n   echo \"=== Env vars in build script: ===\"\n   env | grep IDENTITY || echo \"IDENTITY not in environment\"\n\n   # Layer 3: Signing script\n   echo \"=== Keychain state: ===\"\n   security list-keychains\n   security find-identity -v\n\n   # Layer 4: Actual signing\n   codesign --sign \"$IDENTITY\" --verbose=4 \"$APP\"\n   ```\n\n   **This reveals:** Which layer fails (secrets → workflow ✓, workflow → build ✗)\n\n5. **Trace Data Flow**\n\n   **WHEN error is deep in call stack:**\n\n   See `root-cause-tracing.md` in this directory for the complete backward tracing technique.\n\n   **Quick version:**\n   - Where does bad value originate?\n   - What called this with bad value?\n   - Keep tracing up until you find the source\n   - Fix at source, not at symptom\n\n### Phase 2: Pattern Analysis\n\n**Find the pattern before fixing:**\n\n1. **Find Working Examples**\n   - Locate similar working code in same codebase\n   - What works that's similar to what's broken?\n\n2. **Compare Against References**\n   - If implementing pattern, read reference implementation COMPLETELY\n   - Don't skim - read every line\n   - Understand the pattern fully before applying\n\n3. **Identify Differences**\n   - What's different between working and broken?\n   - List every difference, however small\n   - Don't assume \"that can't matter\"\n\n4. **Understand Dependencies**\n   - What other components does this need?\n   - What settings, config, environment?\n   - What assumptions does it make?\n\n### Phase 3: Hypothesis and Testing\n\n**Scientific method:**\n\n1. **Form Single Hypothesis**\n   - State clearly: \"I think X is the root cause because Y\"\n   - Write it down\n   - Be specific, not vague\n\n2. **Test Minimally**\n   - Make the SMALLEST possible change to test hypothesis\n   - One variable at a time\n   - Don't fix multiple things at once\n\n3. **Verify Before Continuing**\n   - Did it work? Yes → Phase 4\n   - Didn't work? Form NEW hypothesis\n   - DON'T add more fixes on top\n\n4. **When You Don't Know**\n   - Say \"I don't understand X\"\n   - Don't pretend to know\n   - Ask for help\n   - Research more\n\n### Phase 4: Implementation\n\n**Fix the root cause, not the symptom:**\n\n1. **Create Failing Test Case**\n   - Simplest possible reproduction\n   - Automated test if possible\n   - One-off test script if no framework\n   - MUST have before fixing\n   - Use the `hive_skill:test-driven-development` skill for writing proper failing tests\n\n2. **Implement Single Fix**\n   - Address the root cause identified\n   - ONE change at a time\n   - No \"while I'm here\" improvements\n   - No bundled refactoring\n\n3. **Verify Fix**\n   - Test passes now?\n   - No other tests broken?\n   - Issue actually resolved?\n\n4. **If Fix Doesn't Work**\n   - STOP\n   - Count: How many fixes have you tried?\n   - If < 3: Return to Phase 1, re-analyze with new information\n   - **If ≥ 3: STOP and question the architecture (step 5 below)**\n   - DON'T attempt Fix #4 without architectural discussion\n\n5. **If 3+ Fixes Failed: Question Architecture**\n\n   **Pattern indicating architectural problem:**\n   - Each fix reveals new shared state/coupling/problem in different place\n   - Fixes require \"massive refactoring\" to implement\n   - Each fix creates new symptoms elsewhere\n\n   **STOP and question fundamentals:**\n   - Is this pattern fundamentally sound?\n   - Are we \"sticking with it through sheer inertia\"?\n   - Should we refactor architecture vs. continue fixing symptoms?\n\n   **Discuss with your human partner before attempting more fixes**\n\n   This is NOT a failed hypothesis - this is a wrong architecture.\n\n## Red Flags - STOP and Follow Process\n\nIf you catch yourself thinking:\n- \"Quick fix for now, investigate later\"\n- \"Just try changing X and see if it works\"\n- \"Add multiple changes, run tests\"\n- \"Skip the test, I'll manually verify\"\n- \"It's probably X, let me fix that\"\n- \"I don't fully understand but this might work\"\n- \"Pattern says X but I'll adapt it differently\"\n- \"Here are the main problems: [lists fixes without investigation]\"\n- Proposing solutions before tracing data flow\n- **\"One more fix attempt\" (when already tried 2+)**\n- **Each fix reveals new problem in different place**\n\n**ALL of these mean: STOP. Return to Phase 1.**\n\n**If 3+ fixes failed:** Question the architecture (see Phase 4.5)\n\n## your human partner's Signals You're Doing It Wrong\n\n**Watch for these redirections:**\n- \"Is that not happening?\" - You assumed without verifying\n- \"Will it show us...?\" - You should have added evidence gathering\n- \"Stop guessing\" - You're proposing fixes without understanding\n- \"Ultrathink this\" - Question fundamentals, not just symptoms\n- \"We're stuck?\" (frustrated) - Your approach isn't working\n\n**When you see these:** STOP. Return to Phase 1.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Issue is simple, don't need process\" | Simple issues have root causes too. Process is fast for simple bugs. |\n| \"Emergency, no time for process\" | Systematic debugging is FASTER than guess-and-check thrashing. |\n| \"Just try this first, then investigate\" | First fix sets the pattern. Do it right from the start. |\n| \"I'll write test after confirming fix works\" | Untested fixes don't stick. Test first proves it. |\n| \"Multiple fixes at once saves time\" | Can't isolate what worked. Causes new bugs. |\n| \"Reference too long, I'll adapt the pattern\" | Partial understanding guarantees bugs. Read it completely. |\n| \"I see the problem, let me fix it\" | Seeing symptoms ≠ understanding root cause. |\n| \"One more fix attempt\" (after 2+ failures) | 3+ failures = architectural problem. Question pattern, don't fix again. |\n\n## Quick Reference\n\n| Phase | Key Activities | Success Criteria |\n|-------|---------------|------------------|\n| **1. Root Cause** | Read errors, reproduce, check changes, gather evidence | Understand WHAT and WHY |\n| **2. Pattern** | Find working examples, compare | Identify differences |\n| **3. Hypothesis** | Form theory, test minimally | Confirmed or new hypothesis |\n| **4. Implementation** | Create test, fix, verify | Bug resolved, tests pass |\n\n## When Process Reveals \"No Root Cause\"\n\nIf systematic investigation reveals issue is truly environmental, timing-dependent, or external:\n\n1. You've completed the process\n2. Document what you investigated\n3. Implement appropriate handling (retry, timeout, error message)\n4. Add monitoring/logging for future investigation\n\n**But:** 95% of \"no root cause\" cases are incomplete investigation.\n\n## Supporting Techniques\n\nThese techniques are part of systematic debugging and available in this directory:\n\n- **`root-cause-tracing.md`** - Trace bugs backward through call stack to find original trigger\n- **`defense-in-depth.md`** - Add validation at multiple layers after finding root cause\n- **`condition-based-waiting.md`** - Replace arbitrary timeouts with condition polling\n\n**Related skills:**\n- **hive_skill:test-driven-development** - For creating failing test case (Phase 4, Step 1)\n- **hive_skill:verification-before-completion** - Verify fix worked before claiming success\n\n## Real-World Impact\n\nFrom debugging sessions:\n- Systematic approach: 15-30 minutes to fix\n- Random fixes approach: 2-3 hours of thrashing\n- First-time fix rate: 95% vs 40%\n- New bugs introduced: Near zero vs common",
  },
  {
    name: "test-driven-development",
    description: "Use when implementing any feature or bugfix, before writing implementation code",
    template: "# Test-Driven Development (TDD)\n\n## Overview\n\nWrite the test first. Watch it fail. Write minimal code to pass.\n\n**Core principle:** If you didn't watch the test fail, you don't know if it tests the right thing.\n\n**Violating the letter of the rules is violating the spirit of the rules.**\n\n## When to Use\n\n**Always:**\n- New features\n- Bug fixes\n- Refactoring\n- Behavior changes\n\n**Exceptions (ask your human partner):**\n- Throwaway prototypes\n- Generated code\n- Configuration files\n\nThinking \"skip TDD just this once\"? Stop. That's rationalization.\n\n## The Iron Law\n\n```\nNO PRODUCTION CODE WITHOUT A FAILING TEST FIRST\n```\n\nWrite code before the test? Delete it. Start over.\n\n**No exceptions:**\n- Don't keep it as \"reference\"\n- Don't \"adapt\" it while writing tests\n- Don't look at it\n- Delete means delete\n\nImplement fresh from tests. Period.\n\n## Red-Green-Refactor\n\n```dot\ndigraph tdd_cycle {\n    rankdir=LR;\n    red [label=\"RED\\nWrite failing test\", shape=box, style=filled, fillcolor=\"#ffcccc\"];\n    verify_red [label=\"Verify fails\\ncorrectly\", shape=diamond];\n    green [label=\"GREEN\\nMinimal code\", shape=box, style=filled, fillcolor=\"#ccffcc\"];\n    verify_green [label=\"Verify passes\\nAll green\", shape=diamond];\n    refactor [label=\"REFACTOR\\nClean up\", shape=box, style=filled, fillcolor=\"#ccccff\"];\n    next [label=\"Next\", shape=ellipse];\n\n    red -> verify_red;\n    verify_red -> green [label=\"yes\"];\n    verify_red -> red [label=\"wrong\\nfailure\"];\n    green -> verify_green;\n    verify_green -> refactor [label=\"yes\"];\n    verify_green -> green [label=\"no\"];\n    refactor -> verify_green [label=\"stay\\ngreen\"];\n    verify_green -> next;\n    next -> red;\n}\n```\n\n### RED - Write Failing Test\n\nWrite one minimal test showing what should happen.\n\n<Good>\n```typescript\ntest('retries failed operations 3 times', async () => {\n  let attempts = 0;\n  const operation = () => {\n    attempts++;\n    if (attempts < 3) throw new Error('fail');\n    return 'success';\n  };\n\n  const result = await retryOperation(operation);\n\n  expect(result).toBe('success');\n  expect(attempts).toBe(3);\n});\n```\nClear name, tests real behavior, one thing\n</Good>\n\n<Bad>\n```typescript\ntest('retry works', async () => {\n  const mock = jest.fn()\n    .mockRejectedValueOnce(new Error())\n    .mockRejectedValueOnce(new Error())\n    .mockResolvedValueOnce('success');\n  await retryOperation(mock);\n  expect(mock).toHaveBeenCalledTimes(3);\n});\n```\nVague name, tests mock not code\n</Bad>\n\n**Requirements:**\n- One behavior\n- Clear name\n- Real code (no mocks unless unavoidable)\n\n### Verify RED - Watch It Fail\n\n**MANDATORY. Never skip.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test fails (not errors)\n- Failure message is expected\n- Fails because feature missing (not typos)\n\n**Test passes?** You're testing existing behavior. Fix test.\n\n**Test errors?** Fix error, re-run until it fails correctly.\n\n### GREEN - Minimal Code\n\nWrite simplest code to pass the test.\n\n<Good>\n```typescript\nasync function retryOperation<T>(fn: () => Promise<T>): Promise<T> {\n  for (let i = 0; i < 3; i++) {\n    try {\n      return await fn();\n    } catch (e) {\n      if (i === 2) throw e;\n    }\n  }\n  throw new Error('unreachable');\n}\n```\nJust enough to pass\n</Good>\n\n<Bad>\n```typescript\nasync function retryOperation<T>(\n  fn: () => Promise<T>,\n  options?: {\n    maxRetries?: number;\n    backoff?: 'linear' | 'exponential';\n    onRetry?: (attempt: number) => void;\n  }\n): Promise<T> {\n  // YAGNI\n}\n```\nOver-engineered\n</Bad>\n\nDon't add features, refactor other code, or \"improve\" beyond the test.\n\n### Verify GREEN - Watch It Pass\n\n**MANDATORY.**\n\n```bash\nnpm test path/to/test.test.ts\n```\n\nConfirm:\n- Test passes\n- Other tests still pass\n- Output pristine (no errors, warnings)\n\n**Test fails?** Fix code, not test.\n\n**Other tests fail?** Fix now.\n\n### REFACTOR - Clean Up\n\nAfter green only:\n- Remove duplication\n- Improve names\n- Extract helpers\n\nKeep tests green. Don't add behavior.\n\n### Repeat\n\nNext failing test for next feature.\n\n## Good Tests\n\n| Quality | Good | Bad |\n|---------|------|-----|\n| **Minimal** | One thing. \"and\" in name? Split it. | `test('validates email and domain and whitespace')` |\n| **Clear** | Name describes behavior | `test('test1')` |\n| **Shows intent** | Demonstrates desired API | Obscures what code should do |\n\n## Why Order Matters\n\n**\"I'll write tests after to verify it works\"**\n\nTests written after code pass immediately. Passing immediately proves nothing:\n- Might test wrong thing\n- Might test implementation, not behavior\n- Might miss edge cases you forgot\n- You never saw it catch the bug\n\nTest-first forces you to see the test fail, proving it actually tests something.\n\n**\"I already manually tested all the edge cases\"**\n\nManual testing is ad-hoc. You think you tested everything but:\n- No record of what you tested\n- Can't re-run when code changes\n- Easy to forget cases under pressure\n- \"It worked when I tried it\" ≠ comprehensive\n\nAutomated tests are systematic. They run the same way every time.\n\n**\"Deleting X hours of work is wasteful\"**\n\nSunk cost fallacy. The time is already gone. Your choice now:\n- Delete and rewrite with TDD (X more hours, high confidence)\n- Keep it and add tests after (30 min, low confidence, likely bugs)\n\nThe \"waste\" is keeping code you can't trust. Working code without real tests is technical debt.\n\n**\"TDD is dogmatic, being pragmatic means adapting\"**\n\nTDD IS pragmatic:\n- Finds bugs before commit (faster than debugging after)\n- Prevents regressions (tests catch breaks immediately)\n- Documents behavior (tests show how to use code)\n- Enables refactoring (change freely, tests catch breaks)\n\n\"Pragmatic\" shortcuts = debugging in production = slower.\n\n**\"Tests after achieve the same goals - it's spirit not ritual\"**\n\nNo. Tests-after answer \"What does this do?\" Tests-first answer \"What should this do?\"\n\nTests-after are biased by your implementation. You test what you built, not what's required. You verify remembered edge cases, not discovered ones.\n\nTests-first force edge case discovery before implementing. Tests-after verify you remembered everything (you didn't).\n\n30 minutes of tests after ≠ TDD. You get coverage, lose proof tests work.\n\n## Common Rationalizations\n\n| Excuse | Reality |\n|--------|---------|\n| \"Too simple to test\" | Simple code breaks. Test takes 30 seconds. |\n| \"I'll test after\" | Tests passing immediately prove nothing. |\n| \"Tests after achieve same goals\" | Tests-after = \"what does this do?\" Tests-first = \"what should this do?\" |\n| \"Already manually tested\" | Ad-hoc ≠ systematic. No record, can't re-run. |\n| \"Deleting X hours is wasteful\" | Sunk cost fallacy. Keeping unverified code is technical debt. |\n| \"Keep as reference, write tests first\" | You'll adapt it. That's testing after. Delete means delete. |\n| \"Need to explore first\" | Fine. Throw away exploration, start with TDD. |\n| \"Test hard = design unclear\" | Listen to test. Hard to test = hard to use. |\n| \"TDD will slow me down\" | TDD faster than debugging. Pragmatic = test-first. |\n| \"Manual test faster\" | Manual doesn't prove edge cases. You'll re-test every change. |\n| \"Existing code has no tests\" | You're improving it. Add tests for existing code. |\n\n## Red Flags - STOP and Start Over\n\n- Code before test\n- Test after implementation\n- Test passes immediately\n- Can't explain why test failed\n- Tests added \"later\"\n- Rationalizing \"just this once\"\n- \"I already manually tested it\"\n- \"Tests after achieve the same purpose\"\n- \"It's about spirit not ritual\"\n- \"Keep as reference\" or \"adapt existing code\"\n- \"Already spent X hours, deleting is wasteful\"\n- \"TDD is dogmatic, I'm being pragmatic\"\n- \"This is different because...\"\n\n**All of these mean: Delete code. Start over with TDD.**\n\n## Example: Bug Fix\n\n**Bug:** Empty email accepted\n\n**RED**\n```typescript\ntest('rejects empty email', async () => {\n  const result = await submitForm({ email: '' });\n  expect(result.error).toBe('Email required');\n});\n```\n\n**Verify RED**\n```bash\n$ npm test\nFAIL: expected 'Email required', got undefined\n```\n\n**GREEN**\n```typescript\nfunction submitForm(data: FormData) {\n  if (!data.email?.trim()) {\n    return { error: 'Email required' };\n  }\n  // ...\n}\n```\n\n**Verify GREEN**\n```bash\n$ npm test\nPASS\n```\n\n**REFACTOR**\nExtract validation for multiple fields if needed.\n\n## Verification Checklist\n\nBefore marking work complete:\n\n- [ ] Every new function/method has a test\n- [ ] Watched each test fail before implementing\n- [ ] Each test failed for expected reason (feature missing, not typo)\n- [ ] Wrote minimal code to pass each test\n- [ ] All tests pass\n- [ ] Output pristine (no errors, warnings)\n- [ ] Tests use real code (mocks only if unavoidable)\n- [ ] Edge cases and errors covered\n\nCan't check all boxes? You skipped TDD. Start over.\n\n## When Stuck\n\n| Problem | Solution |\n|---------|----------|\n| Don't know how to test | Write wished-for API. Write assertion first. Ask your human partner. |\n| Test too complicated | Design too complicated. Simplify interface. |\n| Must mock everything | Code too coupled. Use dependency injection. |\n| Test setup huge | Extract helpers. Still complex? Simplify design. |\n\n## Debugging Integration\n\nBug found? Write failing test reproducing it. Follow TDD cycle. Test proves fix and prevents regression.\n\nNever fix bugs without a test.\n\n## Testing Anti-Patterns\n\nWhen adding mocks or test utilities, read @testing-anti-patterns.md to avoid common pitfalls:\n- Testing mock behavior instead of real behavior\n- Adding test-only methods to production classes\n- Mocking without understanding dependencies\n\n## Final Rule\n\n```\nProduction code → test exists and failed first\nOtherwise → not TDD\n```\n\nNo exceptions without your human partner's permission.",
  },
  {
    name: "verification-before-completion",
    description: "Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always",
    template: "# Verification Before Completion\n\n## Overview\n\nClaiming work is complete without verification is dishonesty, not efficiency.\n\n**Core principle:** Evidence before claims, always.\n\n**Violating the letter of this rule is violating the spirit of this rule.**\n\n## The Iron Law\n\n```\nNO COMPLETION CLAIMS WITHOUT FRESH VERIFICATION EVIDENCE\n```\n\nIf you haven't run the verification command in this message, you cannot claim it passes.\n\n## The Gate Function\n\n```\nBEFORE claiming any status or expressing satisfaction:\n\n1. IDENTIFY: What command proves this claim?\n2. RUN: Execute the FULL command (fresh, complete)\n3. READ: Full output, check exit code, count failures\n4. VERIFY: Does output confirm the claim?\n   - If NO: State actual status with evidence\n   - If YES: State claim WITH evidence\n5. ONLY THEN: Make the claim\n\nSkip any step = lying, not verifying\n```\n\n## Common Failures\n\n| Claim | Requires | Not Sufficient |\n|-------|----------|----------------|\n| Tests pass | Test command output: 0 failures | Previous run, \"should pass\" |\n| Linter clean | Linter output: 0 errors | Partial check, extrapolation |\n| Build succeeds | Build command: exit 0 | Linter passing, logs look good |\n| Bug fixed | Test original symptom: passes | Code changed, assumed fixed |\n| Regression test works | Red-green cycle verified | Test passes once |\n| Agent completed | VCS diff shows changes | Agent reports \"success\" |\n| Requirements met | Line-by-line checklist | Tests passing |\n\n## Red Flags - STOP\n\n- Using \"should\", \"probably\", \"seems to\"\n- Expressing satisfaction before verification (\"Great!\", \"Perfect!\", \"Done!\", etc.)\n- About to commit/push/PR without verification\n- Trusting agent success reports\n- Relying on partial verification\n- Thinking \"just this once\"\n- Tired and wanting work over\n- **ANY wording implying success without having run verification**\n\n## Rationalization Prevention\n\n| Excuse | Reality |\n|--------|---------|\n| \"Should work now\" | RUN the verification |\n| \"I'm confident\" | Confidence ≠ evidence |\n| \"Just this once\" | No exceptions |\n| \"Linter passed\" | Linter ≠ compiler |\n| \"Agent said success\" | Verify independently |\n| \"I'm tired\" | Exhaustion ≠ excuse |\n| \"Partial check is enough\" | Partial proves nothing |\n| \"Different words so rule doesn't apply\" | Spirit over letter |\n\n## Key Patterns\n\n**Tests:**\n```\n✅ [Run test command] [See: 34/34 pass] \"All tests pass\"\n❌ \"Should pass now\" / \"Looks correct\"\n```\n\n**Regression tests (TDD Red-Green):**\n```\n✅ Write → Run (pass) → Revert fix → Run (MUST FAIL) → Restore → Run (pass)\n❌ \"I've written a regression test\" (without red-green verification)\n```\n\n**Build:**\n```\n✅ [Run build] [See: exit 0] \"Build passes\"\n❌ \"Linter passed\" (linter doesn't check compilation)\n```\n\n**Requirements:**\n```\n✅ Re-read plan → Create checklist → Verify each → Report gaps or completion\n❌ \"Tests pass, phase complete\"\n```\n\n**Agent delegation:**\n```\n✅ Agent reports success → Check VCS diff → Verify changes → Report actual state\n❌ Trust agent report\n```\n\n## Why This Matters\n\nFrom 24 failure memories:\n- your human partner said \"I don't believe you\" - trust broken\n- Undefined functions shipped - would crash\n- Missing requirements shipped - incomplete features\n- Time wasted on false completion → redirect → rework\n- Violates: \"Honesty is a core value. If you lie, you'll be replaced.\"\n\n## When To Apply\n\n**ALWAYS before:**\n- ANY variation of success/completion claims\n- ANY expression of satisfaction\n- ANY positive statement about work state\n- Committing, PR creation, task completion\n- Moving to next task\n- Delegating to agents\n\n**Rule applies to:**\n- Exact phrases\n- Paraphrases and synonyms\n- Implications of success\n- ANY communication suggesting completion/correctness\n\n## The Bottom Line\n\n**No shortcuts for verification.**\n\nRun the command. Read the output. THEN claim the result.\n\nThis is non-negotiable.",
  },
  {
    name: "writing-plans",
    description: "Use when you have a spec or requirements for a multi-step task, before touching code",
    template: "# Writing Plans\n\n## Overview\n\nWrite comprehensive implementation plans assuming the engineer has zero context for our codebase and questionable taste. Document everything they need to know: which files to touch for each task, code, testing, docs they might need to check, how to test it. Give them the whole plan as bite-sized tasks. DRY. YAGNI. TDD. Frequent commits.\n\nAssume they are a skilled developer, but know almost nothing about our toolset or problem domain. Assume they don't know good test design very well.\n\n**Announce at start:** \"I'm using the writing-plans skill to create the implementation plan.\"\n\n**Context:** This should be run in a dedicated worktree (created by brainstorming skill).\n\n**Save plans to:** `docs/plans/YYYY-MM-DD-<feature-name>.md`\n\n## Bite-Sized Task Granularity\n\n**Each step is one action (2-5 minutes):**\n- \"Write the failing test\" - step\n- \"Run it to make sure it fails\" - step\n- \"Implement the minimal code to make the test pass\" - step\n- \"Run the tests and make sure they pass\" - step\n- \"Commit\" - step\n\n## Plan Document Header\n\n**Every plan MUST start with this header:**\n\n```markdown\n# [Feature Name] Implementation Plan\n\n> **For Claude:** REQUIRED SUB-SKILL: Use hive_skill:executing-plans to implement this plan task-by-task.\n\n**Goal:** [One sentence describing what this builds]\n\n**Architecture:** [2-3 sentences about approach]\n\n**Tech Stack:** [Key technologies/libraries]\n\n---\n```\n\n## Task Structure\n\n```markdown\n### Task N: [Component Name]\n\n**Files:**\n- Create: `exact/path/to/file.py`\n- Modify: `exact/path/to/existing.py:123-145`\n- Test: `tests/exact/path/to/test.py`\n\n**Step 1: Write the failing test**\n\n```python\ndef test_specific_behavior():\n    result = function(input)\n    assert result == expected\n```\n\n**Step 2: Run test to verify it fails**\n\nRun: `pytest tests/path/test.py::test_name -v`\nExpected: FAIL with \"function not defined\"\n\n**Step 3: Write minimal implementation**\n\n```python\ndef function(input):\n    return expected\n```\n\n**Step 4: Run test to verify it passes**\n\nRun: `pytest tests/path/test.py::test_name -v`\nExpected: PASS\n\n**Step 5: Commit**\n\n```bash\ngit add tests/path/test.py src/path/file.py\ngit commit -m \"feat: add specific feature\"\n```\n```\n\n## Remember\n- Exact file paths always\n- Complete code in plan (not \"add validation\")\n- Exact commands with expected output\n- Reference relevant skills with @ syntax\n- DRY, YAGNI, TDD, frequent commits\n\n## Execution Handoff\n\nAfter saving the plan, offer execution choice:\n\n**\"Plan complete and saved to `docs/plans/<filename>.md`. Two execution options:**\n\n**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration\n\n**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints\n\n**Which approach?\"**\n\n**If Subagent-Driven chosen:**\n- **REQUIRED SUB-SKILL:** Use hive_skill:subagent-driven-development\n- Stay in this session\n- Fresh subagent per task + code review\n\n**If Parallel Session chosen:**\n- Guide them to open new session in worktree\n- **REQUIRED SUB-SKILL:** New session uses hive_skill:executing-plans",
  }
];
